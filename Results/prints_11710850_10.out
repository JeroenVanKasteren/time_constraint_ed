Rounded t down to nearest multiple of 1/gamma.
J = 2 , D = 50 , s = 8 , gamma = 25.0 , (P= 1000.0 ) , load= 0.5361 
 lambda: [1.3536 1.8554] 
 mu: [0.8791 0.675 ] 
 Target: [1. 1.] 
 r: [1. 1.] 
 c: [1. 1.] 
 s_star: [3.1342 4.8658] 
 rho: [0.4913 0.5649] 
 P(W>D): [0.01700618 0.01590629] 
 Weighted cap_prob: 0.0164 
 W:  0.0051 GB. 
 V:  0.0017 GB.
iter:  0 inner_iter:  -1 , delta:  0.88 , D_min 0.0 , D_max 0.88 , g:  25.0
iter:  100 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.3037
iter:  200 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.293
iter:  300 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2936
iter:  400 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2938
iter:  500 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2939
iter:  600 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2941
iter:  620 inner_iter:  -1 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2941
Value Iteration converged in 620 iterations. g= 3.2941
iter:  0 inner_iter:  0 , delta:  1.8 , D_min 0.06 , D_max 1.85 , g:  54.4438
iter:  0 inner_iter:  100 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.2246
iter:  0 inner_iter:  200 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.204
iter:  0 inner_iter:  300 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1979
iter:  0 inner_iter:  400 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1962
iter:  0 inner_iter:  500 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1956
iter:  0 inner_iter:  600 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1955
iter:  0 inner_iter:  700 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1956
iter:  0 inner_iter:  800 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1956
iter:  0 inner_iter:  820 , delta:  0.0 , D_min 0.06 , D_max 0.06 , g:  3.1956
One-step Policy Improvement converged in 820 iterations. g= 3.1956
