Rounded t down to nearest multiple of 1/gamma.
J = 2 , D = 59 , s = 5 , gamma = 25.0 , (P= 1000.0 ) , load= 0.5046 
 lambda: [0.6392 1.5369] 
 mu: [0.6496 0.9985] 
 Target: [1. 1.] 
 r: [1. 1.] 
 c: [1. 1.] 
 s_star: [2.0051 2.9949] 
 rho: [0.4907 0.5139] 
 P(W>D): [0.07265744 0.01134754] 
 Weighted cap_prob: 0.0294 
 W:  0.0031 GB. 
 V:  0.001 GB.
iter:  0 inner_iter:  -1 , delta:  0.91 , D_min 0.0 , D_max 0.91 , g:  25.0
iter:  100 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1702
iter:  200 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1642
iter:  300 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1629
iter:  400 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1627
iter:  500 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1626
iter:  580 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1625
Value Iteration converged in 580 iterations. g= 2.1625
iter:  0 inner_iter:  0 , delta:  1.66 , D_min 0.04 , D_max 1.7 , g:  47.8929
iter:  0 inner_iter:  100 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1504
iter:  0 inner_iter:  200 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.144
iter:  0 inner_iter:  300 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1418
iter:  0 inner_iter:  400 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1406
iter:  0 inner_iter:  500 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1401
iter:  0 inner_iter:  600 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.14
iter:  0 inner_iter:  700 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.14
iter:  0 inner_iter:  740 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.14
One-step Policy Improvement converged in 740 iterations. g= 2.14
