Rounded t down to nearest multiple of 1/gamma.
J = 2 , D = 193 , s = 2 , gamma = 25.0 , (P= 1000.0 ) , load= 0.4325 
 lambda: [0.3247 0.2276] 
 mu: [0.7072 0.5608] 
 Target: [1. 1.] 
 r: [1. 1.] 
 c: [1. 1.] 
 s_star: [1.1107 0.8893] 
 rho: [0.4133 0.4563] 
 P(W>D): [0.01201484 0.06184201] 
 Weighted cap_prob: 0.0325 
 W:  0.0081 GB. 
 V:  0.0027 GB.
iter:  0 inner_iter:  -1 , delta:  0.97 , D_min 0.0 , D_max 0.97 , g:  25.0
iter:  100 inner_iter:  -1 , delta:  0.01 , D_min 0.0 , D_max 0.01 , g:  0.3745
iter:  200 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.42
iter:  300 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4403
iter:  400 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4558
iter:  500 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4679
iter:  600 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4764
iter:  700 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4816
iter:  800 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4849
iter:  900 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4868
iter:  1000 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.488
iter:  1100 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4888
iter:  1200 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4892
iter:  1300 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4895
iter:  1400 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4897
iter:  1500 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4897
iter:  1580 inner_iter:  -1 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4897
Value Iteration converged in 1580 iterations. g= 0.4897
iter:  0 inner_iter:  0 , delta:  1.45 , D_min 0.01 , D_max 1.46 , g:  37.7495
iter:  0 inner_iter:  100 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4487
iter:  0 inner_iter:  200 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4587
iter:  0 inner_iter:  300 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.463
iter:  0 inner_iter:  400 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4678
iter:  0 inner_iter:  500 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4717
iter:  0 inner_iter:  600 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4746
iter:  0 inner_iter:  700 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4767
iter:  0 inner_iter:  800 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4781
iter:  0 inner_iter:  900 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4791
iter:  0 inner_iter:  1000 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4797
iter:  0 inner_iter:  1100 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4801
iter:  0 inner_iter:  1200 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4804
iter:  0 inner_iter:  1300 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4805
iter:  0 inner_iter:  1400 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4806
iter:  0 inner_iter:  1500 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4806
iter:  0 inner_iter:  1530 , delta:  0.0 , D_min 0.01 , D_max 0.01 , g:  0.4806
One-step Policy Improvement converged in 1530 iterations. g= 0.4806
