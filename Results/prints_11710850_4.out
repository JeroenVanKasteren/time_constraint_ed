Rounded t down to nearest multiple of 1/gamma.
J = 2 , D = 50 , s = 6 , gamma = 25.0 , (P= 1000.0 ) , load= 0.4856 
 lambda: [0.8027 1.398 ] 
 mu: [0.748  0.7597] 
 Target: [1. 1.] 
 r: [1. 1.] 
 c: [1. 1.] 
 s_star: [2.393 3.607] 
 rho: [0.4484 0.5102] 
 P(W>D): [0.0368475  0.01804263] 
 Weighted cap_prob: 0.0249 
 W:  0.0031 GB. 
 V:  0.001 GB.
iter:  0 inner_iter:  -1 , delta:  0.92 , D_min 0.0 , D_max 0.92 , g:  25.0
iter:  100 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.216
iter:  200 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.2129
iter:  300 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.2134
iter:  400 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.2135
iter:  500 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.2138
iter:  530 inner_iter:  -1 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.2137
Value Iteration converged in 530 iterations. g= 2.2137
iter:  0 inner_iter:  0 , delta:  1.68 , D_min 0.04 , D_max 1.72 , g:  47.9813
iter:  0 inner_iter:  100 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1982
iter:  0 inner_iter:  200 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1878
iter:  0 inner_iter:  300 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1842
iter:  0 inner_iter:  400 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1832
iter:  0 inner_iter:  500 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1829
iter:  0 inner_iter:  600 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1829
iter:  0 inner_iter:  680 , delta:  0.0 , D_min 0.04 , D_max 0.04 , g:  2.1829
One-step Policy Improvement converged in 680 iterations. g= 2.1829
