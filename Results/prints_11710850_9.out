Rounded t down to nearest multiple of 1/gamma.
J = 2 , D = 137 , s = 10 , gamma = 25.0 , (P= 1000.0 ) , load= 0.8208 
 lambda: [3.8115 1.4029] 
 mu: [0.974  0.3267] 
 Target: [1. 1.] 
 r: [1. 1.] 
 c: [1. 1.] 
 s_star: [5.2765 4.7235] 
 rho: [0.7416 0.9092] 
 P(W>D): [0.00098001 0.3864532 ] 
 Weighted cap_prob: 0.1047 
 W:  0.0553 GB. 
 V:  0.0184 GB.
iter:  0 inner_iter:  -1 , delta:  0.84 , D_min 0.0 , D_max 0.84 , g:  25.0
iter:  100 inner_iter:  -1 , delta:  0.05 , D_min 0.04 , D_max 0.09 , g:  4.0992
iter:  200 inner_iter:  -1 , delta:  0.02 , D_min 0.07 , D_max 0.09 , g:  4.7529
iter:  300 inner_iter:  -1 , delta:  0.01 , D_min 0.08 , D_max 0.09 , g:  4.8699
iter:  400 inner_iter:  -1 , delta:  0.01 , D_min 0.08 , D_max 0.09 , g:  4.8588
iter:  500 inner_iter:  -1 , delta:  0.01 , D_min 0.08 , D_max 0.08 , g:  4.8598
iter:  600 inner_iter:  -1 , delta:  0.0 , D_min 0.08 , D_max 0.08 , g:  4.8637
iter:  700 inner_iter:  -1 , delta:  0.0 , D_min 0.08 , D_max 0.08 , g:  4.8681
Value Iteration iter: 700 ( -1 ) reached max_time = True , g~ 4.8681
iter:  0 inner_iter:  0 , delta:  3.21 , D_min 0.09 , D_max 3.3 , g:  101.1501
One-step Policy Improvement iter: 0 ( 0 ) reached max_time = True , g~ 101.1501
